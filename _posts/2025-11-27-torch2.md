---
layout: post
title:  "第二章"
date:   2025-11-27 18:00:00 +0800
categories: [pytorch]
tags: [github, jekyll]
---

vscode server在没有代理时才能成功安装，update mode改为none/manual，更新还要重装server（？  
vscode快捷键：ctrl+`打开新终端；F5debug and run；F10next step
## 2.2自动求导
.requires_grad=True，调用.backward()可以自动计算所有梯度，这个张量的梯度累加到.grad属性。y.backward()，y是标量不需传入参数，否则传入一个与y同形的tensor（torch.tensor(形状)）  
手动创建（直接构造而不是计算得到的）的张量.grad_fn为None，.grad_fn表示该tensor由哪个function产生的，.grad表示存储梯度值  
y.backward()计算梯度,print(x.grad)输出$\delta y/\delta x$，grad在反向传播是累加的，清零：x.grad.data.zero_()

```python
x=torch.randn(3,requires_grad=True)# n
print(x)

y=x*2# m
i=0
while y.data.norm()<1000:
    y=y*2
    i=i+1
print(y)
print(i)

v=torch.tensor([0.1,1.0,0.0001],dtype=torch.float)# m
y.backward(v) # 计算雅可比向量积，不直接求矩阵（m$\times n$很大）
print(x.grad)# n
```


y.backward(v)是在计算$x.grad=J^{T}v$，又因为$v^Tt=y^Tv$，所以$\frac{\delta (y^Tv)}{\delta x}=J^Tv$。  
反向传播中的梯度传递（链式法则）:  
设最后一步的梯度向量为：
$v = \frac{\partial \,\text{loss}}{\partial y}$

第二层接收该梯度，计算：
$\frac{\partial \,\text{loss}}{\partial h}
=\left( \frac{\partial y}{\partial h} \right)^{T}\cdot\frac{\partial \,\text{loss}}{\partial y}=J_2^{T} v$，
其中 $J_2$ 是第二层的局部雅可比矩阵。

第一层接收该梯度并计算：
$\frac{\partial \,\text{loss}}{\partial x}=J_1^{T}\left(\frac{\partial \,\text{loss}}{\partial h}\right)$

因此整个反向传播链为：
$\frac{\partial \,\text{loss}}{\partial x}=J_1^{T} J_2^{T}\frac{\partial \,\text{loss}}{\partial y}$

