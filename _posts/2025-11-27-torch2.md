---
layout: post
title:  "torch第二章"
date:   2025-11-27 18:00:00 +0800
categories: [pytorch]
tags: [github, jekyll]
---
## WSL连上vscode
vscode server在没有代理时才能成功安装，update mode改为none/manual，更新还要重装server（？  
vscode快捷键：ctrl+`打开新终端；F5debug and run；F10next step
## 2.2自动求导
.requires_grad=True，调用.backward()可以自动计算所有梯度，这个张量的梯度累加到.grad属性。y.backward()，y是标量不需传入参数，否则传入一个与y同形的tensor（torch.tensor(形状)）  
手动创建（直接构造而不是计算得到的）的张量.grad_fn为None，.grad_fn表示该tensor由哪个function产生的，.grad表示存储梯度值  
y.backward()计算梯度,print(x.grad)输出$\frac{\delta y}{\delta x}$，grad在反向传播是累加的，清零：x.grad.data.zero_()

```python
x=torch.randn(3,requires_grad=True)# n
print(x)

y=x*2# m
i=0
while y.data.norm()<1000:
    y=y*2
    i=i+1
print(y)
print(i)

v=torch.tensor([0.1,1.0,0.0001],dtype=torch.float)# m
y.backward(v) # 计算雅可比向量积，不直接求矩阵（m*n很大）
print(x.grad)# n
```


y.backward(v)是在计算$x.grad=J^{T}v$，又因为$v^Tt=y^Tv$，所以$\frac{\delta (y^Tv)}{\delta x}=J^Tv$。  
反向传播中的梯度传递（链式法则）:  
设最后一步的梯度向量为：
$v = \frac{\partial \,\text{loss}}{\partial y}$

第二层接收该梯度，计算：
$\frac{\partial \,\text{loss}}{\partial h}
=\left( \frac{\partial y}{\partial h} \right)^{T}\cdot\frac{\partial \,\text{loss}}{\partial y}=J_2^{T} v$，
其中 $J_2$ 是第二层的局部雅可比矩阵。

第一层接收该梯度并计算：
$\frac{\partial \,\text{loss}}{\partial x}=J_1^{T}\left(\frac{\partial \,\text{loss}}{\partial h}\right)$

因此整个反向传播链为：
$\frac{\partial \,\text{loss}}{\partial x}=J_1^{T} J_2^{T}\frac{\partial \,\text{loss}}{\partial y}$
## 2.3并行计算简介
cuda是NVIDIA提供的一种GPU并行计算框架，cuda语言实现gpu编程。pytorch中，.cuda()表示将数据从cpu迁移到gpu计算。数据在gpu和cpu中传递耗时较多；简单计算尽量用cpu完成；服务器有多个gpu时应指明用哪块gpu，不设置默认第一块（0号tensor.cuda(0)），可能out of memory。

```python
 #设置在文件最开始部分
import os
os.environ["CUDA_VISIBLE_DEVICE"] = "2" # 设置默认的显卡
```


```python
 CUDA_VISBLE_DEVICE=0,1 python train.py # 使用0，1两块GPU
 ```


并行方法：1.network partitioning，拆分模型在不同的gpu上，但密集任务中gpu通信难达成;2.Intra-layer parallelism，拆分同一层模型（通道数）；3.data parallelism，复制模型，拆分数据（batch），独立计算（forward，backward），汇总梯度，再次更新模型保持多个gpu上一致

单卡训练：model.cuda()模型转移到cuda上，image.cuda()、label.cuda()图片、标签转移到cuda上

多卡训练：  
单机多卡data parallel(DP)，一个优化器，对各gpu梯度求和，在主gpu进行参数更新，再将模型参数广播到其他gpu。容易负载不均衡（输出默认在第一块gpu上

```python
model = Net()
model.cuda()

if torch.cuda.device_count() > 1: # 含有多张GPU的卡
	model = nn.DataParallel(model) # 单机多卡DP训练
    # model = nn.DataParallel(model, device_ids=[0,1])指定gpu编号（或手动指定程序可见gpu设备os.environ["CUDA_VISIBLE_DEVICES"] = "1,2"
```


多机多卡distributed data parallel(DDP)，每个gpu启动一个进程，每个进程有独立的优化器，计算完梯度由rank=0的gpu汇总，再广播到所有进程，各进程用该梯度独立更新参数。模型初始化和更新都一致

进程组概念：GROUP：进程组；WORLD_SIZE：全局进程个数，多机多卡表示机器数量，单机多卡表示gpu数量；RANK:进程序号，用于进程间通信，表示进程优先级；LOCAL_RANK:进程内gpu编号，由torch.distributed.launch内部指定



DDP优势：1.只进行梯度等少量数据交换；2.每个进程包含独立的解释器和全局解释器锁（GIL，保证同一时刻只有一个线程在执行python代码）

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>
