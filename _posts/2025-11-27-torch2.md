---
layout: post
title:  "torch第二章"
date:   2025-11-27 18:00:00 +0800
categories: [pytorch]
tags: [github, jekyll]
---
## WSL连上vscode
vscode server在没有代理时才能成功安装，update mode改为none/manual，更新还要重装server（？  
vscode快捷键：ctrl+`打开新终端；F5debug and run；F10next step
## 2.2自动求导
.requires_grad=True，调用.backward()可以自动计算所有梯度，这个张量的梯度累加到.grad属性。y.backward()，y是标量不需传入参数，否则传入一个与y同形的tensor（torch.tensor(形状)）  
手动创建（直接构造而不是计算得到的）的张量.grad_fn为None，.grad_fn表示该tensor由哪个function产生的，.grad表示存储梯度值  
y.backward()计算梯度,print(x.grad)输出$\frac{\delta y}{\delta x}$，grad在反向传播是累加的，清零：x.grad.data.zero_()

```python
x=torch.randn(3,requires_grad=True)# n
print(x)

y=x*2# m
i=0
while y.data.norm()<1000:
    y=y*2
    i=i+1
print(y)
print(i)

v=torch.tensor([0.1,1.0,0.0001],dtype=torch.float)# m
y.backward(v) # 计算雅可比向量积，不直接求矩阵（m*n很大）
print(x.grad)# n
```


y.backward(v)是在计算$x.grad=J^{T}v$，又因为$v^Tt=y^Tv$，所以$\frac{\delta (y^Tv)}{\delta x}=J^Tv$。  
反向传播中的梯度传递（链式法则）:  
设最后一步的梯度向量为：
$v = \frac{\partial \,\text{loss}}{\partial y}$

第二层接收该梯度，计算：
$\frac{\partial \,\text{loss}}{\partial h}
=\left( \frac{\partial y}{\partial h} \right)^{T}\cdot\frac{\partial \,\text{loss}}{\partial y}=J_2^{T} v$，
其中 $J_2$ 是第二层的局部雅可比矩阵。

第一层接收该梯度并计算：
$\frac{\partial \,\text{loss}}{\partial x}=J_1^{T}\left(\frac{\partial \,\text{loss}}{\partial h}\right)$

因此整个反向传播链为：
$\frac{\partial \,\text{loss}}{\partial x}=J_1^{T} J_2^{T}\frac{\partial \,\text{loss}}{\partial y}$
## 2.3并行计算简介
cuda是NVIDIA提供的一种GPU并行计算框架，cuda语言实现gpu编程。pytorch中，.cuda()表示将数据从cpu迁移到gpu计算。数据在gpu和cpu中传递耗时较多；简单计算尽量用cpu完成；服务器有多个gpu时应指明用哪块gpu，不设置默认第一块（0号tensor.cuda(0)），可能out of memory。

```python
 #设置在文件最开始部分
import os
os.environ["CUDA_VISIBLE_DEVICE"] = "2" # 设置默认的显卡
```


```python
 CUDA_VISBLE_DEVICE=0,1 python train.py # 使用0，1两块GPU
 ```


并行方法：1.network partitioning，拆分模型在不同的gpu上，但密集任务中gpu通信难达成;2.Intra-layer parallelism，拆分同一层模型（通道数）；3.data parallelism，复制模型，拆分数据（batch），独立计算（forward，backward），汇总梯度，再次更新模型保持多个gpu上一致

单卡训练：model.cuda()模型转移到cuda上，image.cuda()、label.cuda()图片、标签转移到cuda上

多卡训练：  
单机多卡data parallel(DP)，一个优化器，对各gpu梯度求和，在主gpu进行参数更新，再将模型参数广播到其他gpu。容易负载不均衡（输出默认在第一块gpu上

```python
model = Net()
model.cuda()

if torch.cuda.device_count() > 1: # 含有多张GPU的卡
	model = nn.DataParallel(model) # 单机多卡DP训练
    # model = nn.DataParallel(model, device_ids=[0,1])指定gpu编号（或手动指定程序可见gpu设备os.environ["CUDA_VISIBLE_DEVICES"] = "1,2"
```


多机多卡distributed data parallel(DDP)，每个gpu启动一个进程，每个进程有独立的优化器，计算完梯度由rank=0的gpu汇总，再广播到所有进程，各进程用该梯度独立更新参数。模型初始化和更新都一致  
准备阶段：将各模型复制到gpu上一份，并将shuffle后的batch数据等分到不同的gpu上  
训练阶段：
- 前向传播，损失函数的计算在每个gpu上独立执行
- 反向传播，各进程通过ALL-Reduce与其他进程通讯，交换各自的梯度a,b,c表示不同参数），从而获得所有进程的平均梯度
- 各gpu使用平均梯度在所有gpu上执行梯度下降，更新自己的参数
![alt text](/images/ring-all_reduce.jpg "Ring-All Reduce")

进程组概念：GROUP：进程组；WORLD_SIZE：全局进程个数，多机多卡表示机器数量，单机多卡表示gpu数量；RANK:进程序号，用于进程间通信，表示进程优先级；LOCAL_RANK:进程内gpu编号，由torch.distributed.launch内部指定

DDP代码编写流程（粗略）：1.在使用 distributed 包的任何其他函数之前，需要使用 init_process_group 初始化进程组，同时初始化 distributed 包；2.使用 torch.nn.parallel.DistributedDataParallel 创建 分布式模型 DDP(model, device_ids=device_ids)；3.使用 torch.utils.data.distributed.DistributedSampler 创建 DataLoader；4.使用启动工具 torch.distributed.launch 在每个主机上执行一次脚本，开始训练

DDP后端通信，单机多卡直接nccl，多机多卡尝试nccl，可gloo代替

DDP启动方式
- mp.spawn()
- torchrnn
- torch.distributed.launch（即将被淘汰）

程序修改（mp版本）
- 导入关键包
   - dist：多卡通讯
   - mp：DDP程序启动
   - GradScaler：混合精度训练
   - DistributedSampler：数据采样
   - DDP：模型传递
 - 定义关键函数
   - init_ddp(local_rank)，使用nccl协议进行后端通信，并用env作为初始化方法，local_rank()=dist.get_rank()当前进程序号，world_size=dist.get_world_size()总进程数
   - reduce_tensor(tensor)，对多个进程的计算结果汇总
   - get_ddp_generator(seed)，对每个进程使用随机种子，增加训练随机性
  - 程序入口
   - if __name__=='__main__'中，parser.add_argument定义几个gpu，环境变量等等（torchrnn会自动控制一些），mp.spawn()函数启动
  - main()，参数列表添加额外参数local_rank，调用init_ddp()初始化；调用convert_sync_batchnorm()实现BN层同步；调用DistributedDataParallel()实现模型封装；调用GradScaler()混合精度训练，作为参数传入train()函数；避免副本重复执行，if local_rank==0约束，打印epoch；dist.destroy_process_group()消除进程组
  - get_dataloader()
   - 训练，设置train_sampler随机采样，get_ddp_generator()
   - 测试，设置test_sampler为顺序采样
  - train()
  - validate()
  - 

DDP优势：1.只进行梯度等少量数据交换；2.每个进程包含独立的解释器和全局解释器锁（GIL，保证同一时刻只有一个线程在执行python代码）

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>
