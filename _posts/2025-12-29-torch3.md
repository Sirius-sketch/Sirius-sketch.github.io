---
layout: post
title:  "torch第三章"
date:   2025-12-29 18:00:00 +0800
categories: [pytorch]
tags: [github, jekyll]
---
## 3.2基本配置
导入常用包

```python
import os 
import numpy as np 
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torch.optim as optimizer
```


超参数
```python
batch_size = 16
lr = 1e-4
max_epochs = 100
```


调用gpu
```python
# 方案一：使用os.environ，这种情况如果使用GPU不需要设置
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' # 指明调用的GPU为0,1号

# 方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可
device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu") # 指明调用的GPU为1号
```


## 3.3数据读入
核心机制：采用 Dataset + DataLoader 的组合。

Dataset：用户自定义类，继承 torch.utils.data.Dataset类并实现三个函数：__init__（参数传入与定义）、__getitem__（按索引读入并处理样本）、__len__（返回数据总量）。

DataLoader：负责批处理（Batching）、洗牌（Shuffle）、多进程加速（num_workers）等

```python
import torch
from torchvision import datasets
train_data = datasets.ImageFolder(train_path, transform=data_transform)
val_data = datasets.ImageFolder(val_path, transform=data_transform)
```


ImageFolder类用于读取按一定结构存放的图片的目录（path随影目录，每个子目录下放一类图片）

```python

import os
import pandas as pd
from torchvision.io import read_image

class MyDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label
```


os.path.join() method concatenates path components into a single, platform-appropriate path. 在linux中file_path=os.path.join(a,b)是a/b，在windows中是a\b

```python
from torch.utils.data import DataLoader

train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, shuffle=True, drop_last=True)
val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=4, shuffle=False)
```


Windows下num_workers通常设置为0（如果在 Windows 下将 num_workers设置为大于0的数，必须将数据加载和训练代码放在 if __name__ == '__main__': 语句块下，否则会触发多进程递归错误），linux下4、8  
用Dataloader来批量读取数据。drop_last是最后一部分没有达到批数的样本不再参与训练

```python
import matplotlib.pyplot as plt
images, labels = next(iter(val_loader))
print(images.shape)
plt.imshow(images[0].transpose(1,2,0))
plt.show()
```


查看加载的数据。iter()从list,tuple,string等创建可迭代对象，next()从迭代对象中一个个取出元素。

```python
my_list = [10, 20, 30]
my_iterator = iter(my_list) 
print(next(my_iterator)) # Output: 10
print(next(my_iterator)) # Output: 20
print(next(my_iterator)) # Output: 30
# print(next(my_iterator)) # This would raise StopIteration
print(next(my_iterator, "No more items")) # Output: No more items
```


## 3.4模型构建
基类：所有神经网络模块必须继承 torch.nn.Module。

核心步骤：在 __init__ 中定义层（如 nn.Linear, nn.Conv2d），在 forward 中定义前向传播逻辑。

典型组件：卷积层、池化层、全连接层等。

多继承:  
super()公式的本质：super(当前类,self)在self的mro列表中找到当前列的后一个类  
mro生成规则：1.子类优先于父类；2.左右顺序；  
3.单调性（如果在 B 的继承链中 X 在 Y 之前，那么在 B 的子类 D 的继承链中，X 也必须在 Y 之前）  

```python
class A:
    def __init__(self):
        print("A")

class B(A):
    def __init__(self):
        super().__init__()
        print("B")

class C(A):
    def __init__(self):
        super().__init__()
        print("C")

class D(B, C):
    def __init__(self):
        super().__init__()
        print("D")
D()
print(D.mro())  # 打印方法解析顺序
```


```python
import torch
from torch import nn

class MLP(nn.Module):
  # 声明带有模型参数的层，这里声明了两个全连接层
  def __init__(self, **kwargs):
    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数
    super(MLP, self).__init__(**kwargs)
    self.hidden = nn.Linear(784, 256)
    self.act = nn.ReLU()
    self.output = nn.Linear(256,10)
    
   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出
  def forward(self, x):
    o = self.act(self.hidden(x))
    return self.output(o)   
```


Module的子类可以是一个层、模型或模型的一部分  
用Module来自定义层

```python
import torch
from torch import nn

#不含参数的层
class MyLayer(nn.Module):
    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)
    def forward(self, x):
        return x - x.mean()  

#含模型参数的层
class MyListDense(nn.Module):
    def __init__(self):
        super(MyListDense, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for _ in range(3)])#执行三次，ParameterList里面3个4*4的可学习参数
        self.params.append(nn.Parameter(torch.randn(4, 1)))

    def forward(self, x):
        for i in range(len(self.params)):
            x = torch.mm(x, self.params[i])
        return x
net = MyListDense()
print(net)

#卷积层
# 卷积运算（二维互相关）
def corr2d(X, K): 
    h, w = K.shape
    X, K = X.float(), K.float()
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()
    return Y

# 二维卷积层
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super(Conv2D, self).__init__()
        self.weight = nn.Parameter(torch.randn(kernel_size))
        self.bias = nn.Parameter(torch.randn(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias


#池化层
def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
    return Y
```


*对应位置相乘，@矩阵乘法，结果都是矩阵，求和要.sum()  
num_flat_features(x)把每个样本拉成特征向量

```python
def num_flat_features(self, x):
    size = x.size()[1:]   # 去掉 batch 维度
    num_features = 1
    for s in size:
        num_features *= s
    return num_features
```

训练流程：  
- 定义包含一些可学习参数的神经网络
```python
params = list(net.parameters())#参数可用net.parameters()返回
print(len(params))
print(params[0].size())  # conv1的权重
```


- 在输入数据集上迭代
- 通过网络处理输入
- 计算 loss（输出和正确答案的距离）
- 将梯度反向传播给网络的参数
```python
net.zero_grad()
out.backward(torch.randn(1, 10))
```


- 更新网络的权重，一般使用一个简单的规则：weight = weight - learning_rate * gradient

torch.nn只支持小批量处理（mini-batches）。整个 torch.nn 包只支持小批量样本的输入，不支持单个样本的输入。即输入一定是一个四维向量nSamples x nChannels x Height x Width。如果是一个单独的样本，只需要使用input.unsqueeze(0) 来添加一个“假的”批大小维度。

## 3.5模型初始化
重要性：避免全0初始化导致梯度消失/对称性问题。

方法：使用 torch.nn.init 库（如 xavier_normal_, kaiming_normal_ 等）。通常通过遍历 model.modules() 并结合 isinstance 判断层类型来进行批量初始化。
初始化方法：  
|初始化方法|常用名称|适用场景|核心思想|
|---|---|---|---|
|xavier_uniform_|Glorot初始化|tanh、sigmoid、softmax|保持输入和输出的方差一致，防止梯度消失|
|kaiming_uniform_|He初始化|relu、leaky|考虑到ReLU会使一半神经元失活，将方差放大$\sqrt{2}$倍|
|constant_|常数初始化|偏置|通常偏置初始化为0|
选择初始化方法主要取决于使用的激活函数  
uniform_均匀分布，参数a,b，采样上下限，normal_正态分布，参数mean,std，均值和标准差  

计算增益torch.nn.init.calculate_gain，为了让信号在多层网络中既不爆炸也不消失，需要根据激活函数的特性，给初始化权重乘上一个系数(gain)来调节。  
比如：Relu把负半轴切掉了，丢了一半信息，所以把剩下的信号增强$\sqrt{2}$来补偿


torch.nn.init,后缀有_原地更改向量值（pytorch中，末尾下划线的函数表示in-place原地操作  
isinstance()判断模块是否属于某一类型  

```python
import torch
import torch.nn as nn

conv=nn.Conv2d(1,3,3)#输入通道1个，输出通道3个（3个卷积核），每个卷积核大小3*3
isinstance(conv,nn.Conv2d) # True

conv.weight.data#查看随机初始化参数
#(3，1，3，3)
# tensor(
# [
#   [
#     [
#       [ ... , ... , ... ],
#       [ ... , ... , ... ],
#       [ ... , ... , ... ]
#     ]
#   ],

#   [
#     [
#       [ ... , ... , ... ],
#       [ ... , ... , ... ],
#       [ ... , ... , ... ]
#     ]
#   ],

#   [
#     [
#       [ ... , ... , ... ],
#       [ ... , ... , ... ],
#       [ ... , ... , ... ]
#     ]
#   ]
# ]
# )
```


Conv2d的权重是4维张量(out_channels,in_channels,kernel_h,kernel_w)

初始化函数封装

```python
#版本1
def initialize_weights(model):
	for m in model.modules():
		# 判断是否属于Conv2d
		if isinstance(m, nn.Conv2d):
			torch.nn.init.zeros_(m.weight.data)
			# 判断是否有偏置
			if m.bias is not None:
				torch.nn.init.constant_(m.bias.data,0.3)
		elif isinstance(m, nn.Linear):
			torch.nn.init.normal_(m.weight.data, 0.1)
			if m.bias is not None:
				torch.nn.init.zeros_(m.bias.data)
		elif isinstance(m, nn.BatchNorm2d):
			m.weight.data.fill_(1) 		 
			m.bias.data.zeros_()
# 模型的定义
class MLP(nn.Module):
  ...
mlp = MLP()
mlp.apply(initialize_weights)
# 或者initialize_weights(mlp)
print(mlp.hidden.weight.data)

#版本2
#一般不适用全0初始化	
def init_weights(m):
    # 使用 isinstance 判断层类型
    if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
    elif isinstance(m, nn.Linear):
        nn.init.xavier_normal_(m.weight)
        nn.init.constant_(m.bias, 0)

# 使用 model.apply 自动遍历所有子模块
model.apply(init_weights)
```
