---
layout: post
title:  "torch第三章"
date:   2025-12-29 18:00:00 +0800
categories: [pytorch]
tags: [github, jekyll]
---
## 3.2基本配置
导入常用包

```python
import os 
import numpy as np 
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torch.optim as optimizer
```


超参数
```python
batch_size = 16
lr = 1e-4
max_epochs = 100
```


调用gpu
```python
# 方案一：使用os.environ，这种情况如果使用GPU不需要设置
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' # 指明调用的GPU为0,1号

# 方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可
device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu") # 指明调用的GPU为1号
```


## 3.3数据读入
核心机制：采用 Dataset + DataLoader 的组合。

Dataset：用户自定义类，继承 torch.utils.data.Dataset类并实现三个函数：__init__（参数传入与定义）、__getitem__（按索引读入并处理样本）、__len__（返回数据总量）。

DataLoader：负责批处理（Batching）、洗牌（Shuffle）、多进程加速（num_workers）等

```python
import torch
from torchvision import datasets
train_data = datasets.ImageFolder(train_path, transform=data_transform)
val_data = datasets.ImageFolder(val_path, transform=data_transform)
```


ImageFolder类用于读取按一定结构存放的图片的目录（path随影目录，每个子目录下放一类图片）

```python

import os
import pandas as pd
from torchvision.io import read_image

class MyDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label
```


os.path.join() method concatenates path components into a single, platform-appropriate path. 在linux中file_path=os.path.join(a,b)是a/b，在windows中是a\b

```python
from torch.utils.data import DataLoader

train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, shuffle=True, drop_last=True)
val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=4, shuffle=False)
```


Windows下num_workers通常设置为0（如果在 Windows 下将 num_workers设置为大于0的数，必须将数据加载和训练代码放在 if __name__ == '__main__': 语句块下，否则会触发多进程递归错误），linux下4、8  
用Dataloader来批量读取数据。drop_last是最后一部分没有达到批数的样本不再参与训练

```python
import matplotlib.pyplot as plt
images, labels = next(iter(val_loader))
print(images.shape)
plt.imshow(images[0].transpose(1,2,0))
plt.show()
```


查看加载的数据。iter()从list,tuple,string等创建可迭代对象，next()从迭代对象中一个个取出元素。

```python
my_list = [10, 20, 30]
my_iterator = iter(my_list) 
print(next(my_iterator)) # Output: 10
print(next(my_iterator)) # Output: 20
print(next(my_iterator)) # Output: 30
# print(next(my_iterator)) # This would raise StopIteration
print(next(my_iterator, "No more items")) # Output: No more items
```


## 3.4模型构建
基类：所有神经网络模块必须继承 torch.nn.Module。

核心步骤：在 __init__ 中定义层（如 nn.Linear, nn.Conv2d），在 forward 中定义前向传播逻辑。

典型组件：卷积层、池化层、全连接层等。

多继承:  
super()公式的本质：super(当前类,self)在self的mro列表中找到当前列的后一个类  
mro生成规则：1.子类优先于父类；2.左右顺序；  
3.单调性（如果在 B 的继承链中 X 在 Y 之前，那么在 B 的子类 D 的继承链中，X 也必须在 Y 之前）  

```python
class A:
    def __init__(self):
        print("A")

class B(A):
    def __init__(self):
        super().__init__()
        print("B")

class C(A):
    def __init__(self):
        super().__init__()
        print("C")

class D(B, C):
    def __init__(self):
        super().__init__()
        print("D")
D()
print(D.mro())  # 打印方法解析顺序
```


```python
import torch
from torch import nn

class MLP(nn.Module):
  # 声明带有模型参数的层，这里声明了两个全连接层
  def __init__(self, **kwargs):
    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数
    super(MLP, self).__init__(**kwargs)
    self.hidden = nn.Linear(784, 256)
    self.act = nn.ReLU()
    self.output = nn.Linear(256,10)
    
   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出
  def forward(self, x):
    o = self.act(self.hidden(x))
    return self.output(o)   
```


Module的子类可以是一个层、模型或模型的一部分  
用Module来自定义层

```python
import torch
from torch import nn

#不含参数的层
class MyLayer(nn.Module):
    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)
    def forward(self, x):
        return x - x.mean()  

#含模型参数的层
class MyListDense(nn.Module):
    def __init__(self):
        super(MyListDense, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for _ in range(3)])#执行三次，ParameterList里面3个4*4的可学习参数
        self.params.append(nn.Parameter(torch.randn(4, 1)))

    def forward(self, x):
        for i in range(len(self.params)):
            x = torch.mm(x, self.params[i])
        return x
net = MyListDense()
print(net)

#卷积层
# 卷积运算（二维互相关）
def corr2d(X, K): 
    h, w = K.shape
    X, K = X.float(), K.float()
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()
    return Y

# 二维卷积层
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super(Conv2D, self).__init__()
        self.weight = nn.Parameter(torch.randn(kernel_size))
        self.bias = nn.Parameter(torch.randn(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias


#池化层
def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
    return Y
```


*对应位置相乘，@矩阵乘法，结果都是矩阵，求和要.sum()  
num_flat_features(x)把每个样本拉成特征向量

```python
def num_flat_features(self, x):
    size = x.size()[1:]   # 去掉 batch 维度
    num_features = 1
    for s in size:
        num_features *= s
    return num_features
```

训练流程：  
- 定义包含一些可学习参数的神经网络
```python
params = list(net.parameters())#参数可用net.parameters()返回
print(len(params))
print(params[0].size())  # conv1的权重
```


- 在输入数据集上迭代
- 通过网络处理输入
- 计算 loss（输出和正确答案的距离）
- 将梯度反向传播给网络的参数
```python
net.zero_grad()
out.backward(torch.randn(1, 10))
```


- 更新网络的权重，一般使用一个简单的规则：weight = weight - learning_rate * gradient

torch.nn只支持小批量处理（mini-batches）。整个 torch.nn 包只支持小批量样本的输入，不支持单个样本的输入。即输入一定是一个四维向量nSamples x nChannels x Height x Width。如果是一个单独的样本，只需要使用input.unsqueeze(0) 来添加一个“假的”批大小维度。

## 3.5模型初始化
重要性：避免全0初始化导致梯度消失/对称性问题。

方法：使用 torch.nn.init 库（如 xavier_normal_, kaiming_normal_ 等）。通常通过遍历 model.modules() 并结合 isinstance 判断层类型来进行批量初始化。
初始化方法：  
|初始化方法|常用名称|适用场景|核心思想|
|---|---|---|---|
|xavier_uniform_|Glorot初始化|tanh、sigmoid、softmax|保持输入和输出的方差一致，防止梯度消失|
|kaiming_uniform_|He初始化|relu、leaky|考虑到ReLU会使一半神经元失活，将方差放大$\sqrt{2}$倍|
|constant_|常数初始化|偏置|通常偏置初始化为0|
选择初始化方法主要取决于使用的激活函数  
uniform_均匀分布，参数a,b，采样上下限，normal_正态分布，参数mean,std，均值和标准差  

计算增益torch.nn.init.calculate_gain，为了让信号在多层网络中既不爆炸也不消失，需要根据激活函数的特性，给初始化权重乘上一个系数(gain)来调节。  
比如：Relu把负半轴切掉了，丢了一半信息，所以把剩下的信号增强$\sqrt{2}$来补偿


torch.nn.init,后缀有_原地更改向量值（pytorch中，末尾下划线的函数表示in-place原地操作  
isinstance()判断模块是否属于某一类型  

```python
import torch
import torch.nn as nn

conv=nn.Conv2d(1,3,3)#输入通道1个，输出通道3个（3个卷积核），每个卷积核大小3*3
isinstance(conv,nn.Conv2d) # True

conv.weight.data#查看随机初始化参数
#(3，1，3，3)
# tensor(
# [
#   [
#     [
#       [ ... , ... , ... ],
#       [ ... , ... , ... ],
#       [ ... , ... , ... ]
#     ]
#   ],

#   [
#     [
#       [ ... , ... , ... ],
#       [ ... , ... , ... ],
#       [ ... , ... , ... ]
#     ]
#   ],

#   [
#     [
#       [ ... , ... , ... ],
#       [ ... , ... , ... ],
#       [ ... , ... , ... ]
#     ]
#   ]
# ]
# )
```


Conv2d的权重是4维张量(out_channels,in_channels,kernel_h,kernel_w)

初始化函数封装

```python
#版本1
def initialize_weights(model):
	for m in model.modules():
		# 判断是否属于Conv2d
		if isinstance(m, nn.Conv2d):
			torch.nn.init.zeros_(m.weight.data)
			# 判断是否有偏置
			if m.bias is not None:
				torch.nn.init.constant_(m.bias.data,0.3)
		elif isinstance(m, nn.Linear):
			torch.nn.init.normal_(m.weight.data, 0.1)
			if m.bias is not None:
				torch.nn.init.zeros_(m.bias.data)
		elif isinstance(m, nn.BatchNorm2d):
			m.weight.data.fill_(1) 		 
			m.bias.data.zeros_()
# 模型的定义
class MLP(nn.Module):
  ...
mlp = MLP()
mlp.apply(initialize_weights)
# 或者initialize_weights(mlp)
print(mlp.hidden.weight.data)

#版本2
#一般不适用全0初始化	
def init_weights(m):
    # 使用 isinstance 判断层类型
    if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
    elif isinstance(m, nn.Linear):
        nn.init.xavier_normal_(m.weight)
        nn.init.constant_(m.bias, 0)

# 使用 model.apply 自动遍历所有子模块
model.apply(init_weights)
```


## 3.6损失函数
| 损失函数名称 (Class Name)  | 适用任务类型 (Task Type)          | 核心公式概念 (Formula Concept)                | 关键参数 (Key Parameters)                                         | 输入/目标形状要求 (Shape Notes)                        |
| :------------------: | :-------------------------: | :-------------------------------------: | :-----------------------------------------------------------: | :--------------------------------------------: |
| BCELoss              | 二分类 (Binary Classification) | −∑\[ylogx+(1−y)log(1−x)\]               | weight: 样本权重reduction: 'mean'/'sum'/'none'                    | Input $\\in $ (Sigmoid后)Target $\\in $ (Float) |
| BCEWithLogitsLoss    | 二分类 (推荐)                    | Sigmoid + BCELoss (LogSumExp优化)         | pos\_weight: 正样本权重weight, reduction                           | Input: Logits (无激活)Target: Float               |
| CrossEntropyLoss     | 多分类 (Multi-class)           | −log(∑ex\[j\]ex\[class\]​)              | weight: 类别权重ignore\_index: 忽略Paddinglabel\_smoothing          | Input: (N,C) LogitsTarget: (N) Long Index      |
| NLLLoss              | 多分类 (配合LogSoftmax)          | −x\[class\]                             | weight, ignore\_index                                         | Input: Log-Probabilities                       |
| MSELoss              | 回归 (Regression)             | (x−y)2                                  | reduction: 'mean' (默认)                                        | 对异常值敏感Input/Target 形状一致                        |
| L1Loss               | 回归 (Regression)             | $ \|x - y\|\_1 $                        | $x - y$ (This seems to be a formula concept, not a parameter) | $                                              |
| SmoothL1Loss         | 回归 (目标检测)                   | 小误差平方，大误差线性 (Huber)                     | beta: 阈值 (默认1.0)reduction                                     | 防止梯度爆炸，平滑收敛                                    |
| PoissonNLLLoss       | 计数回归 (Count Regression)     | $ e^x−x⋅y $                             | log\_input: 是否为对数输入full: 是否计算Stirling近似                       | Target 服从泊松分布                                  |
| KLDivLoss            | 分布匹配 (Distillation)         | $\\sum p(x) \\log \\frac{p(x)}{q(x)}$   | reduction: 推荐 'batchmean'                                     | Input: Log-ProbsTarget: Probs                  |
| MarginRankingLoss    | 排序学习 (Ranking)              | max(0,−y(x1​−x2​)+m)                    | margin: 间隔reduction                                           | Target ∈{1,−1}                                 |
| TripletMarginLoss    | 度量学习 (FaceID)               | max(d(a,p)−d(a,n)+m,0)                  | margin, p (范数度数)                                              | 输入三元组 (Anchor, Pos, Neg)                       |
| CosineEmbeddingLoss  | 相似度学习 (Similarity)          | $1−\\cos$ (相似) vs max(0,$\\cos−m$) (不似) | margin                                                        | 基于向量夹角而非距离                                     |
| CTCLoss              | 序列对齐 (ASR/OCR)              | 所有可能对齐路径概率之和                            | blank: 空白符索引zero\_infinity                                    | Input: (T,N,C)Target: (N,S)                    |
| MultiLabelMarginLoss | 多标签分类                       | 铰链损失变体                                  | reduction                                                     | 适用于一个样本属多个类                                    |
| SoftMarginLoss       | 二分类优化                       | $\\log(1+e^{-y⋅x})$                     | reduction                                                     | 类似 Logistic Loss                               |

四类损失函数
### 1.回归损失（连续数值预测）
均方误差$MSE=\frac{1}{N}\sum_{n=1}^{N}(x_n-y_n)^2$，对异常值敏感  
参数：
- reduction:
  - mean（默认）：返回batch中所有样本损失的平均值
  - sum:返回总和
  - none:返回形状为（N，*）的张量，包含每个样本的独立损失，可对特定样本加权

平均绝对误差$L1Loss=\frac{1}{N}\sum_{n=1}^{N}|x_n-y_n|$，

Huber Loss:  
$loss(x,y)=\frac{1}{N}\sum_{i=1}^Nz_i$  
$$z_i=
\begin{cases}
0.5(x_i-y_i)^2 & \text{if}|x_i-y_i|<\beta \\
|x_i-y_i|-o.5\beta & \text{otherwise} \\
\end{cases}
$$
$\beta$默认1.0，保证在0点附近的平滑性，误差较大时限制了异常值产生的梯度幅度防止梯度爆炸

### 2.分类损失
二分类交叉熵BCELoss，输入数据x时经过sigmoid激活的概率值  
$l_n=-[y_n\cdot log x_n+(1-y_n)\cdot log(1-x_n)]$  
直接使用sigmoid+BCELoss存在数值稳定性风险，概率趋于0或1时可能计算溢出，BCEWithLogitsLoss函数内部将sigmoid和BCELoss合并，利用Log-Sum-Exp在数学上消除了溢出风险（？）

多分类交叉熵CrossEntropyLoss，输入的是未归一化的得分（logits）（不用手动softmax），实际是将nn.LogSoftmax和nn.NLLLoss集成在一个类中。input为(N,C),output为(N)  
$$
loss(x)=-log(\frac{e^{x[class]}}{\sum_je^{x[j]}})=-x[class]+log(\sum_je^{x[j]})
$$

负对数似然损失NLLLoss，本质上是CrossEntropyLoss的后半部分，不包含softmax和log操作，假设输入是LogSoftmax处理过的对数概率  
$l_n=-x_{n,y_n}$即取目标类别对应的对数概率值的负数

### 3.排序、度量嵌入损失
不直接预测具体的值或类别，而是学习样本在特征空间中的相对关系，在人脸识别、推荐系统、图像检索中很重要

MarginRankingLoss，用于学习两个输入x_1和x_2的相对排序。  
给定标签$y\in\{1,-1\}$，若y=1，则希望$x_1>x_2$且至少大于一个margin，反之亦然  
$loss(x_1,x_2,y)=max(0,-y\cdot(x_1-x_2)+margin)$  
这是一种铰链损失函数（Hinge Loss）变体，如果排序正确且差异超过margin则损失为0，否则产生线性惩罚

TripletMarginLoss三元组损失，是FaceNet等人脸识别的核心，需要三个输入：
- Anchor(a):锚点样本
- Positive(p):与锚点同类的样本
- Negative(n):与锚点不同类的样本
优化目标是将Anchor拉近Positive同时推开Negative  
$L(a,p,n)=max{d(a,p)-d(a,n)+margin,0}$  
难点在于找难例（hard negatives），即虽然是异类但是看起来像同类的样本，有利于提升模型鉴别能力

CosineEmbeddingLoss余弦相似度损失
- 若标签y=1相似，优化目标是$1-cos(x_1,x_2)$，即夹角趋于0
- 若标签y=-1不相似，优化目标是$max(0,cos(x_1,x_2)-margin)$，即夹角大于某个阈值

### 4.序列和其他损失
CTCLoss，连接主义时序分类，解决对齐难题，在语音识别和光学字符识别中，输入序列通常大于输出序列

 PoissonNLLLoss，泊松分布，预测目标是计数数据且事件发生独立随机

 KLDivLoss，KL散度，知识蒸馏中应用广泛。用于连续分布的距离度量，并且对离散采用的连续输出空间分布进行回归通常很有用

## 3.7训练与评估
训练过程
```python
model.train() #训练状态
for data,label in train_loader: #读取DataLoader中的全部数据
data,label = data.cuda(),label.cuda() #放到GPU上
optimizer.zero_grad() #开始用当前批次数据做训练时，先将优化器梯度置0
output=model(data) #将data送入模型中训练
loss=criterion(output,label) #用预先定义的criiterion计算损失函数
loss.backward() #将loss反向传播回网络
optimizer.step() #使用优化器更新模型参数
```


验证过程
- torch.no_grad
- model改成eval
- 不需要将优化器梯度置零
- 不需要将loss反向回传到网络
- 不需要更新optimizer

```python
#训练过程示例
def train(epoch):
    model.train()
    train_loss = 0
    for data, label in train_loader:
        data, label = data.cuda(), label.cuda()
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, label)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()*data.size(0)
    train_loss = train_loss/len(train_loader.dataset)
		print('Epoch: {} \tTraining Loss: {:.6f}'.format(epoch, train_loss))

#验证过程示例
def val(epoch):       
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for data, label in val_loader:
            data, label = data.cuda(), label.cuda()
            output = model(data)
            preds = torch.argmax(output, 1)
            loss = criterion(output, label)
            val_loss += loss.item()*data.size(0)
            running_accu += torch.sum(preds == label.data)
    val_loss = val_loss/len(val_loader.dataset)
    print('Epoch: {} \tTraining Loss: {:.6f}'.format(epoch, val_loss))
```


## 3.9优化器
pytorch优化算法均继承于Optimizer；每个优化器都是一个类，要进行实例化才能使用，optim = torch.optim.SGD(net.parameters(),lr=lr)；optimizer在一个神经网络的epoch中需要实现梯度置零和梯度更新两个步骤
```python
optimizer = torch.optim.SGD(net.parameters(), lr=1e-5)
for epoch in range(EPOCH):
	...
	optimizer.zero_grad()  #梯度置零
	loss = ...             #计算loss
	loss.backward()        #BP反向传播
	optimizer.step()       #梯度更新
```


Optimizer三个属性
```python
class Optimizer(object):
    def __init__(self, params, defaults):        
        self.defaults = defaults #存储优化器的超参数
        self.state = defaultdict(dict) #参数缓存
        self.param_groups = [] #管理的参数组，是一个list，其中每个元素是一个字典，顺序是params，lr，momentum，dampening，weight_decay，nesterov
```


Optimizer方法
- zero_grad():清空所管理参数的梯度
  - 叶子张量：requires_grad=True但grad_fn=None，不是由其他算子生成，但是任何对它的操作-都会被记录在新的计算图中 
  - 推荐使用set_to_none=True
```python
def zero_grad(self, set_to_none: bool = False): #False把梯度设为0，True把梯度设为None不存在
    for group in self.param_groups:
        for p in group['params']: #从字典group中取出key为'params'的值，p是模型参数torch.nn.Parameter
        #参数分组：不同的参数采用不同的学习率、优化策略
            if p.grad is not None:  #梯度不为空
                if set_to_none: #直接把梯度对象删掉
                    p.grad = None 
                else: #梯度对象还留着，会被重复使用
                    if p.grad.grad_fn is not None: #计算图是算出来的
                        p.grad.detach_() #断开历史（从计算图中剥离出来变成叶子张量，停止追踪，requires_grad属性设为false
                    else:
                        p.grad.requires_grad_(False) #变成静态常量，不用再追踪梯度
                    p.grad.zero_() #梯度设置为0
```


参数分组
 ```python
self.param_groups == [
    {'params': [...], 'lr': ...},
    {'params': [...], 'lr': ...},
    ...
]
```


- step():执行一步梯度更新
- add_param_group():添加参数组
```python
def add_param_group(self, param_group):
    assert isinstance(param_group, dict), "param group must be a dict" #检查传入是dict
    # 检查类型是否为tensor
    params = param_group['params']
    if isinstance(params, torch.Tensor):#若params本身是一个Tensor，则包一层list[]
        param_group['params'] = [params]
    elif isinstance(params, set): #set是无序的而优化器更新参数依赖顺序一致性
        raise TypeError('optimizer parameters need to be organized in ordered collections, but '
                        'the ordering of tensors in sets will change between runs. Please use a list instead.')
    else: #其他可迭代对象统一变成list
        param_group['params'] = list(params)
    #检查每个参数本身是否合法
    for param in param_group['params']:
        if not isinstance(param, torch.Tensor): #必须是tensor
            raise TypeError("optimizer can only optimize Tensors, "
                            "but one of the params is " + torch.typename(param))
        if not param.is_leaf: #必须是leaf tensor
            raise ValueError("can't optimize a non-leaf Tensor")

    for name, default in self.defaults.items(): #补齐优化器需要的超参数
        if default is required and name not in param_group: #如果必须给的参数没给
            raise ValueError("parameter group didn't specify a value of required optimization parameter " +
                             name)
        else:#其他没给的参数用默认值
            param_group.setdefault(name, default)

    params = param_group['params']
    if len(params) != len(set(params)):#检查一个param_group里面有没有重复的参数
        warnings.warn("optimizer contains a parameter group with duplicate parameters; "
                      "in future, this will cause an error; "
                      "see github.com/PyTorch/PyTorch/issues/40967 for more information", stacklevel=3)
    param_set = set() #不同param_group间不能有重叠
    for group in self.param_groups:
        param_set.update(set(group['params']))

    if not param_set.isdisjoint(set(param_group['params'])):
        raise ValueError("some parameters appear in more than one parameter group")
# 添加参数
    self.param_groups.append(param_group)
```



- load_state_dict():加载状态参数字典，
```python
def load_state_dict(self, state_dict):
    # 深拷贝输入的字典状态，避免修改原始数据
    state_dict = deepcopy(state_dict)
    # 验证参数组数量是否一致
    groups = self.param_groups
    saved_groups = state_dict['param_groups']

    if len(groups) != len(saved_groups):
        raise ValueError("loaded state dict has a different number of "
                         "parameter groups")
    # 检查每个参数组中的参数个数是否一致
    param_lens = (len(g['params']) for g in groups)
    saved_lens = (len(g['params']) for g in saved_groups)
    if any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):
        raise ValueError("loaded state dict contains a parameter group "
                         "that doesn't match the size of optimizer's group")

    # 建立映射关系
    id_map = {old_id: p for old_id, p in
              zip(chain.from_iterable((g['params'] for g in saved_groups)),
                  chain.from_iterable((g['params'] for g in groups)))}
    # 恢复状态字典中的状态数据
    state = defaultdict(dict)
    for k, v in state_dict['state'].items():
        if k in id_map:
            #k是整数索引，需要转换回tensor对象
            param = id_map[k]
            state[param] = cast(param, v)#转换数据格式
        else:
            state[k] = v

    # 更新参数组
    def update_group(group, new_group):
        #将新的参数组中的整数索引替换为当前的tensor
       ...
    param_groups = [
        update_group(g, ng) for g, ng in zip(groups, saved_groups)]
    # 设置优化器状态
    self.__setstate__({'state': state, 'param_groups': param_groups})
```


- state_dict()，获取优化器当前状态信息字典
```python
def state_dict(self):
    # Save order indices instead of Tensors
    param_mappings = {}
    start_index = 0
    #给每个参数编号
    def pack_group(group):
        # group 是一个字典，包含 'params' 和其他超参数（如 'lr'、'momentum' 等）
        packed = {} #新的空字典
        start_index = 0
        # 构建映射：遍历所有参数组，给每个参数分配一个整数索引
        for group in self.param_groups:
            for p in group['params']:  # p 是 tensor 参数对象
                param_mappings[id(p)] = start_index  # 将参数的内存地址映射到整数
                start_index += 1
        # 处理 'params' 键
        packed['params'] = [param_mappings[id(p)] for p in group['params']]#id(p)获取参数p的内存地址
        # 这里 param_mappings[id(p)] 就是参数 p 的整数索引
    
        # 复制其他键值对（lr、momentum 等），保持不变
        for key, value in group.items():
            if key != 'params':
                packed[key] = value
    
        return packed
    param_groups = [pack_group(g) for g in self.param_groups]
    #例如：原来
    # param_groups = [
    # {'params': [tensor_param1, tensor_param2], 'lr': 0.001},
    # {'params': [tensor_param3], 'lr': 0.0001}
    # ]
    # 转换后
    # param_groups = [
    # {'params': [0, 1], 'lr': 0.001},
    # {'params': [2], 'lr': 0.0001}
    # ]
    # Remap state to use order indices as keys
    packed_state = {(param_mappings[id(k)] if isinstance(k, torch.Tensor) else k): v
                    for k, v in self.state.items()} #遍历self.state的所有键值对，如果键k是tensor则转换为整数索引，否则保持不变；全部项都放入packed_state
    #state每个参数的动量等缓存数据
    return {
        'state': packed_state,
        'param_groups': param_groups,
    }
```


<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };
</script>
